# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eIaBckrD-NKZk0MkARTHCtCs81a4Zt17

#  Is AI Sexist?  
## Understanding Bias in Support Vector Machines (SVM)

---
####*A little context:*
In 2018, Amazon discontinued an experimental AI hiring system after discovering that the model was systematically rating male candidates higher than female candidates. The system had learned this behaviour simply because it was trained on historical recruitment data, data that contained more successful male applicants than female ones. As a result, the AI reproduced and amplified existing societal biases, even though gender was never explicitly included as an input feature.


**This real-world example illustrates a fundamental problem in machine learning:
models do not become biased on their own, they learn biases directly from the data they are given.**

---

In this notebook, I explore how machine-learning models can learn biased behaviour when the dataset itself contains imbalances

I use a synthetic dataset inspired by the Amazon hiring case, where male candidates were over-represented in historical data.

I train three Support Vector Machine (SVM) models:

- Linear kernel  
- Polynomial kernel  
- RBF kernel  

Then I evaluate:
- overall accuracy  
- accuracy for male vs female groups  
- how a balanced SVM can reduce unfair behaviour  

This notebook follows a simple structure:
1. Create the dataset  
2. Add a realistic bias  
3. Train several SVM models  
4. Analyse fairness  
5. Improve fairness using a bal

#  1. Import required libraries
"""

# IMPORTS

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    ConfusionMatrixDisplay
)
import matplotlib.pyplot as plt

np.random.seed(42)

"""#  2. Creating the synthetic dataset

The goal is not to use real CVs but to **simulate the statistical patterns** found in biased hiring systems.

We artificially create:

- 700 male samples  
- 300 female samples  
- non-linear feature distributions  
- a circular decision boundary  
- a small advantage for male samples (to mimic Amazon’s issue)

This allows full control over the bias and makes the experiment easy to visualise.

"""

# SYNTHETIC DATASET

n_male = 700
n_female = 300

# --- Male group: two clusters (non-linear) ---
male_cluster1 = np.random.randn(n_male // 2, 2) * 0.6 + np.array([2, 2])
male_cluster2 = np.random.randn(n_male // 2, 2) * 0.7 + np.array([-2, -1])
male = np.vstack([male_cluster1, male_cluster2])

# --- Female group: elongated cluster ---
female = np.random.randn(n_female, 2)
female[:, 0] *= 2.2
female[:, 1] += 1.5

# Combine
X = np.vstack([male, female])
gender = np.array([1] * n_male + [0] * n_female)  # 1=male, 0=female

"""#  3. Preview of the dataset
Below is a quick look at the structure of the generated dataset.

"""

# NON-LINEAR TARGET + SMALL GENDER BIAS

r = np.sqrt(X[:, 0]**2 + X[:, 1]**2)
base_target = (r < 3).astype(int)     # circular boundary

bias = (gender * 0.25) + np.random.randn(len(gender)) * 0.25
y = ((base_target + bias) > 0.5).astype(int)

df = pd.DataFrame({
    "feature1": X[:, 0],
    "feature2": X[:, 1],
    "gender": gender,
    "target": y
})

df.head()

"""The dataset used in this project is synthetic and has been designed to simulate a realistic fairness problem.

It contains two demographic groups, male and female, each represented by clusters in a two-dimensional feature space.

Male samples form two compact clusters, while female samples are more spread out along one axis.

 A slight bias is added to the target labels so that male samples are slightly more likely to receive a positive outcome.

This controlled structure allows us to reproduce a situation similar to real-world bias:
the model appears accurate overall but performs unevenly across groups.
"""

# --- GRAPH: Gender Distribution ---

plt.figure(figsize=(6,4))
df.gender.value_counts().sort_index().plot(
    kind="bar",
    color=["cornflowerblue", "lightcoral"],
    edgecolor="black"
)

plt.xticks([0,1], ["Female", "Male"], rotation=0)
plt.title("Gender Distribution in the Dataset")
plt.ylabel("Number of Samples")
plt.grid(axis="y", alpha=0.3)
plt.show()

"""This graph shows the gender imbalance in the dataset, with male samples being more numerous than female samples.

This imbalance is one of the main sources of bias, because the model will naturally learn patterns that appear more frequently in the majority group.

# 4. Train–test split and normalisation

I separate the dataset into:
- 70%: training
- 30%: testing

I also apply standard scaling because SVM models benefit from normalised inputs.
"""

# TRAIN/TEST SPLIT + NORMALISATION

X = df[["feature1", "feature2"]]   # REMOVE gender from features
y = df["target"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""#  5. Training SVM models with different kernels

I train three models:
- **Linear kernel**: can only draw a straight line
- **Polynomial kernel (degree 3)**: can draw curved boundaries
- **RBF kernel**: the most flexible, handles complex shapes

This allows me to compare how each kernel behaves on biased and non-linear data.

"""

# TRAIN THREE SVM KERNELS

kernels = {
    "Linear": SVC(kernel="linear"),
    "Polynomial (degree=3)": SVC(kernel="poly", degree=3),
    "RBF": SVC(kernel="rbf")
}

models = {}
preds = {}

for name, model in kernels.items():
    print(f"\n==================== {name} ====================")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    models[name] = model
    preds[name] = y_pred

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred, zero_division=0))

"""The Linear SVM fails to detect class 0, the Polynomial SVM improves but still struggles with minority samples, while the RBF SVM correctly captures the nonlinear structure of the data and achieves strong, balanced performance across both classes.

#  6. Bias analysis: accuracy per gender

A model can be accurate overall while being unfair to a subgroup.

Here, I compute:
- accuracy for male samples  
- accuracy for female samples  

This helps identify hidden bias.
"""

# BIAS ANALYSIS — accuracy per gender

gender_test = df.loc[y_test.index, "gender"].values

def group_accuracy(y_true, y_pred, gender_array, value):
    mask = (gender_array == value)
    return accuracy_score(y_true[mask], y_pred[mask])

for name, y_pred in preds.items():
    print(f"\n===== Bias Analysis for {name} Kernel =====")
    acc_male = group_accuracy(y_test.values, y_pred, gender_test, 1)
    acc_female = group_accuracy(y_test.values, y_pred, gender_test, 0)

    print("Accuracy (Male):  ", acc_male)
    print("Accuracy (Female):", acc_female)

"""The Linear model shows a strong gender gap, the Polynomial model reduces this disparity but still favors male samples, while the RBF model achieves the highest accuracy with the smallest difference between male and female performance, indicating a more balanced and fair behaviour."""

# --- Accuracy by Gender Before Balancing (Linear Kernel) ---

male_acc_before = group_accuracy(y_test.values, preds["Linear"], gender_test, 1)
female_acc_before = group_accuracy(y_test.values, preds["Linear"], gender_test, 0)

groups = ["Male", "Female"]
accuracies = [male_acc_before, female_acc_before]

plt.figure(figsize=(6, 5))
bars = plt.bar(groups, accuracies, edgecolor="black", alpha=0.8)

# percentage labels
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,
             f"{height*100:.1f}%", ha="center", fontsize=12)

plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Accuracy by Gender Before Balancing (Linear Kernel)")
plt.grid(axis="y", linestyle="--", alpha=0.3)

plt.show()

"""This plot shows that the Linear SVM performs unevenly across gender groups before any fairness correction.

The model is more accurate for male samples (79.0%) than for female samples (62.2%).

This gap reflects the imbalance in the training data and illustrates how the model learns biased behaviour from the dataset rather than from its design.

#  7. Confusion matrices for each kernel

Confusion matrices show how many samples were correctly or incorrectly classified.

They help visualise:
- where the model is failing  
- whether one group is getting more errors than the other
"""

# CONFUSION MATRICES

fig, axes = plt.subplots(1, 3, figsize=(16, 4))

for ax, (name, y_pred) in zip(axes, preds.items()):
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax, colorbar=False)
    ax.set_title(name)

plt.tight_layout()
plt.show()

"""# Interpretation of the Confusion Matrices

1. **Linear SVM**

The model predicts almost everything as class 1.

- True class 0 → all 78 samples are misclassified

- True class 1 → 222 samples correctly classified

→ This shows that Linear SVM completely fails to separate class 0, leading to a major imbalance in predictions.

**Conclusion:** Linear SVM is too simple for this dataset and produces a heavily biased decision boundary.



------------------------------------------------------

2. **Polynomial SVM (degree = 3)**

- True class 0 → 14 correct, 64 misclassified

- True class 1 → all 222 correctly classified

→ The polynomial kernel improves slightly over the linear model, but still struggles to detect class 0.

**Conclusion:** Although more flexible, the polynomial kernel still favors class 1 and fails to correctly identify many class-0 samples.

------------------------------------------------------
3. **RBF SVM**

- True class 0 → 64 correct, 14 misclassified

- True class 1 → 216 correct, 6 misclassified

→ The RBF kernel performs best: it correctly identifies the majority of samples from both classes and is the only model that captures the non-linear structure of the data.

**Conclusion:** The RBF kernel provides the best balance between sensitivity to class 0 and accuracy on class 1, making it the most reliable model before applying fairness corrections.

#  8. Training a balanced SVM (fairness correction)

I use an RBF SVM with *class balancing enabled*.

This makes errors on minority groups (female samples) more important during training.

The result is usually:
- slightly lower male accuracy
- significantly higher female accuracy  
- more fairness overall
"""

# BALANCED SVM (RBF Kernel)

balanced = SVC(kernel="rbf", class_weight="balanced")
balanced.fit(X_train, y_train)

y_balanced = balanced.predict(X_test)

print("\n=== Balanced RBF Model ===")
print("Accuracy:", accuracy_score(y_test, y_balanced))
print(classification_report(y_test, y_balanced, zero_division=0))

"""The Balanced RBF model achieves high overall accuracy while significantly improving recall for class 0, showing that reweighting the classes makes the classifier both more accurate and more fair toward the minority class.

#  9. Before vs After Balancing: Final Fairness Comparison

This is the key takeaway:
- the original RBF model learns the bias found in the dataset  
- the balanced model corrects some unfair behaviour  
- AI is not “sexist” by itself — it learns from the data
"""

# Before vs After accuracy per gender
male_before = accuracy_score(y_test[gender_test == 1], preds["Linear"][gender_test == 1])
female_before = accuracy_score(y_test[gender_test == 0], preds["Linear"][gender_test == 0])


male_after = accuracy_score(y_test[gender_test == 1], y_balanced[gender_test == 1])
female_after = accuracy_score(y_test[gender_test == 0], y_balanced[gender_test == 0])

print("\n=== Linear Kernel (Before Balancing) ===")
print("Male Accuracy:   ", male_before)
print("Female Accuracy: ", female_before)

print("\n=== Linear Kernel (After Balancing) ===")
print("Male Accuracy:   ", male_after)
print("Female Accuracy: ", female_after)

# --- SIDE-BY-SIDE GRAPHS FOR MALE AND FEMALE BEFORE/AFTER BALANCING ---

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# ======= Graph 1: Male =======
axes[0].bar(["Before", "After"], [male_before, male_after],
            color=["steelblue", "darkorange"], edgecolor="black", alpha=0.85)

# Add % labels
for i, val in enumerate([male_before, male_after]):
    axes[0].text(i, val + 0.01, f"{val*100:.1f}%", ha="center", fontsize=11)

axes[0].set_ylim(0, 1)
axes[0].set_title("Male Accuracy\nLinear Before vs Balanced RBF After")
axes[0].set_ylabel("Accuracy")
axes[0].grid(axis="y", linestyle="--", alpha=0.3)

# ======= Graph 2: Female =======
axes[1].bar(["Before", "After"], [female_before, female_after],
            color=["lightcoral", "mediumseagreen"], edgecolor="black", alpha=0.85)

# Add % labels
for i, val in enumerate([female_before, female_after]):
    axes[1].text(i, val + 0.01, f"{val*100:.1f}%", ha="center", fontsize=11)

axes[1].set_ylim(0, 1)
axes[1].set_title("Female Accuracy\nLinear Before vs Balanced RBF After")
axes[1].set_ylabel("Accuracy")
axes[1].grid(axis="y", linestyle="--", alpha=0.3)

plt.tight_layout()
plt.show()

"""These side-by-side plots compare gender-specific accuracy before and after applying class balancing.

The Linear SVM (Before) shows a clear performance gap: the model is more accurate for male samples than for female samples.
After training a Balanced RBF SVM, the accuracy becomes more equal across groups.

This demonstrates that class balancing helps reduce unfair behaviour learned from the dataset.

"""

# --- GRAPH: Accuracy per SVM Model ---

linear_acc = accuracy_score(y_test, preds["Linear"])
poly_acc = accuracy_score(y_test, preds["Polynomial (degree=3)"])
rbf_acc = accuracy_score(y_test, preds["RBF"])
balanced_acc = accuracy_score(y_test, y_balanced)

accuracies = {
    "Linear": linear_acc,
    "Polynomial": poly_acc,
    "RBF": rbf_acc,
    "Balanced RBF": balanced_acc
}

plt.figure(figsize=(8,5))
plt.bar(
    accuracies.keys(),
    accuracies.values(),
    color=["gray", "skyblue", "deepskyblue", "mediumseagreen"],
    edgecolor="black"
)

plt.ylim(0,1)
plt.title("Overall Accuracy per SVM Model")
plt.ylabel("Accuracy")
plt.grid(axis="y", alpha=0.3)
plt.show()

"""This graph compares the performance of different SVM models.

The Linear and Polynomial kernels perform moderately, while the RBF model achieves high accuracy.

The Balanced RBF slightly reduces male accuracy but increases female accuracy, resulting in a fairer and still highly accurate model.

#  10. Conclusion

In this notebook, I showed that:

- Linear SVM performs poorly on non-linear data  
- Polynomial SVM improves but still shows bias  
- RBF SVM performs best overall  
- The dataset imbalance leads to different accuracies for men and women  
- A balanced SVM reduces unfair behaviour

This experiment demonstrates that **AI learns the patterns it is given**.
If the dataset is biased, the model will reflect that bias, unless we correct it or at least mitigate its impact.
"""