# ML-assignement

# Is AI Sexist? Understanding How Support Vector Machines Learn Bias from Data

This repository contains the code and notebook for a tutorial exploring whether artificial intelligence (AI) can appear sexist when trained on biased data.  
The project is inspired by the real Amazon hiring case, where an internal machine-learning system unintentionally downgraded CVs associated with women.  
The goal is to demonstrate that AI models are not inherently biased; they learn the patterns and imbalances present in the data.

## Overview

The tutorial includes:
- creation of a synthetic dataset with gender imbalance and a slight hidden bias,
- training of three Support Vector Machine (SVM) models (Linear, Polynomial, RBF),
- evaluation of accuracy for male and female samples,
- application of class balancing to reduce unfair behaviour,
- visualisations showing how bias appears and how it can be mitigated.

## Key Concepts

- dataset imbalance  
- label bias and indirect discrimination  
- kernel behaviour in SVMs  
- performance differences across demographic groups  
- fairness correction using class weights  

